{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mental-infrastructure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.12\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-literacy",
   "metadata": {},
   "source": [
    "need to install py2neo, requests, spacy, en_core_web_sm. Then also get an eventregistry.org api key\n",
    "\n",
    "Remove key before git commit. Put key back before running.\n",
    "\n",
    "install pandas for dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-airplane",
   "metadata": {},
   "source": [
    "Following along with this: https://medium.com/analytics-vidhya/build-a-knowledge-graph-using-neo4j-eb7490799f4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "excited-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "from py2neo import database, Graph, Node, Relationship\n",
    "import re\n",
    "import en_core_web_sm\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "article_uri = dict()\n",
    "URL = \"http://eventregistry.org/api/v1/article/getArticles\"\n",
    "KEYWORDS = [[\"antiquities trade\", \"illicit antiquities\", \"illegal antiquties\", \"art trade\", \"art market\"]]\n",
    "\n",
    "\n",
    "def clear_graph(graph):\n",
    "    print(\"[+] Clearing previous graph\")\n",
    "    # graph.cypher.execute(\"MATCH (A) -[R] -> () DELETE A, R\")\n",
    "    # graph.cypher.execute(\"MATCH (A) DELETE A\")\n",
    "    graph.delete_all()\n",
    "    file_name = \"Entities/entities.json\"\n",
    "    if os.path.isfile(file_name):\n",
    "        os.remove(file_name)\n",
    "\n",
    "\n",
    "def get_all_entites():\n",
    "    entities = dict()\n",
    "\n",
    "    entities[\"news_sources\"] = dict()\n",
    "    entities[\"authors\"] = dict()\n",
    "    entities[\"languages\"] = dict()\n",
    "    entities[\"organisations\"] = dict()\n",
    "    entities[\"locations\"] = dict()\n",
    "    entities[\"people\"] = dict()\n",
    "    entities[\"articles\"] = dict()\n",
    "\n",
    "    file_name = \"Entities/entities.json\"\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name) as f:\n",
    "            entities = json.load(f)\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def get_articles_request(keywords, num_articles=100):\n",
    "    print(\"[+] Getting articles for keywords:\", keywords)\n",
    "    articles = []\n",
    "\n",
    "    \n",
    "    # for keywords in KEYWORDS:\n",
    "    payload = {\"action\": \"getArticles\", \"keyword\": keywords, \"articlesPage\": 1, \"articlesSortBy\": \"date\",\n",
    "               \"articlesSortByAsc\": False, \"articlesArticleBodyLen\": -1, \"resultType\": \"articles\",\n",
    "               \"dataType\": [\"news\", \"pr\"], \"apiKey\": \"\",\n",
    "               \"forceMaxDataTimeWindow\": 31,\n",
    "               \"articlesCount\": num_articles}\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", URL, headers=headers, data=json.dumps(payload))\n",
    "    try:\n",
    "        articles = json.loads(response.text.encode('utf8'))\n",
    "    except:\n",
    "        print(\"Exception occured for keyword:\", keywords)\n",
    "        return list()\n",
    "    # print(articles.keys())\n",
    "    print(type(articles))\n",
    "    print(\"Articles request len\", len(articles[\"articles\"][\"results\"]))\n",
    "    return articles[\"articles\"][\"results\"]\n",
    "\n",
    "\n",
    "def get_articles(num_articles=100):\n",
    "    curr_date = datetime.today().strftime('%d-%m-%Y')\n",
    "    #curr_date = \"29 -05-2020\"\n",
    "    file_name = \"Daily_Articles/\" + curr_date + \".pickle\"\n",
    "\n",
    "    if not os.path.isfile(file_name):\n",
    "        print(\"\\tCould not find articles at: {file_name}, requesting\".format(file_name=file_name))\n",
    "        with open(file_name, \"wb\") as handle:\n",
    "            articles = list()\n",
    "            for keywords in KEYWORDS[1]:\n",
    "                articles += get_articles_request(keywords, num_articles)\n",
    "            articles_dict = dict()\n",
    "            articles_dict[\"articles\"] = {\"results\":articles}\n",
    "            pickle.dump(articles_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            return articles_dict #i changed this b/c of twitter guy\n",
    "    else:\n",
    "        print(\"\\tFound articles at: {file_name}\".format(file_name=file_name))\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "\n",
    "\n",
    "def update_entities(entities):\n",
    "    print(\"[+] Updating entities\")\n",
    "\n",
    "    # with open('Entities/entities.pkl', 'wb') as output:\n",
    "    #     pickle.dump(entities, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    file_name = \"Entities/entities.json\"\n",
    "    with open(file_name, 'w') as fp:\n",
    "        json.dump(entities, fp)\n",
    "\n",
    "\n",
    "def get_random_number():\n",
    "    return random.randint(100000000, 1000000000)\n",
    "\n",
    "\n",
    "def check_wiki_page(query):\n",
    "    try:\n",
    "        result = requests.get('https://en.wikipedia.org/wiki/{0}'.format(query), verify=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if result.status_code == 200:  # the article exists\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def create_graph(articles, entities, graph):\n",
    "    count_ = 0\n",
    "    articles_uri = entities[\"articles\"]\n",
    "    news_sources = entities[\"news_sources\"]\n",
    "    languages = entities[\"languages\"]\n",
    "    authors = entities[\"authors\"]\n",
    "    people = entities[\"people\"]\n",
    "    locations = entities[\"locations\"]\n",
    "    organisations = entities[\"organisations\"]\n",
    "\n",
    "    new_articles = []\n",
    "\n",
    "    print(\"[+] Creating nodes and relationships for languages, articles, authors and content-sources\")\n",
    "\n",
    "    for article_ in articles:#[\"articles\"][\"results\"]:\n",
    "        count_ += 1\n",
    "        if article_[\"uri\"] not in articles_uri:\n",
    "            new_articles.append(article_)\n",
    "            print(\"\\tEntering data for article:\", article_[\"uri\"], \"- title:\", article_[\"title\"])\n",
    "\n",
    "            # Creating Nodes\n",
    "            article_pk = get_random_number()\n",
    "            article_node = Node(\"Article\", pk=article_pk, name=article_['title'], sentiment=article_[\"sentiment\"],\n",
    "                                text=article_[\"body\"],\n",
    "                                dtype=article_[\"dataType\"], published_on=article_[\"dateTimePub\"], uri=article_[\"uri\"],\n",
    "                                url=article_[\"url\"])\n",
    "\n",
    "            articles_uri[article_[\"uri\"]] = (article_pk, article_node)\n",
    "\n",
    "            # Entering article\n",
    "            if article_[\"source\"][\"title\"] in news_sources:\n",
    "                source_node_pk = news_sources[article_[\"source\"][\"title\"]][0]\n",
    "                query = \"Match(m:`Content-Source`) where m.pk={0} Return m\".format(source_node_pk)\n",
    "                source_node = graph.evaluate(query)\n",
    "            else:\n",
    "                source_node_pk = get_random_number()\n",
    "                source_node = Node(\"Content-Source\", pk=source_node_pk, name=article_[\"source\"][\"title\"],\n",
    "                                   uri=article_[\"source\"][\"uri\"])\n",
    "\n",
    "            rel_article_source = Relationship(article_node, \"publishedBy\", source_node)\n",
    "            graph.create(rel_article_source)\n",
    "            news_sources[article_[\"source\"][\"title\"]] = (source_node_pk, source_node)\n",
    "\n",
    "            #Entering language\n",
    "            if article_[\"lang\"] in languages:\n",
    "                lang_pk = languages[article_[\"lang\"]][0]\n",
    "                query = \"Match(m:`Language`) where m.pk={0} Return m\".format(lang_pk)\n",
    "                lang_node = graph.evaluate(query)\n",
    "            else:\n",
    "                lang_pk = get_random_number()\n",
    "                lang_node = Node(\"Language\", pk=lang_pk, name=article_[\"lang\"])\n",
    "            \n",
    "            rel_article_lang = Relationship(article_node, \"language\", lang_node)\n",
    "            graph.create(rel_article_lang)\n",
    "            languages[article_[\"lang\"]] = (lang_pk, lang_node)\n",
    "\n",
    "            # Entering authors\n",
    "            for author in article_[\"authors\"]:\n",
    "                if author[\"name\"] in authors:\n",
    "                    author_pk = authors[author[\"name\"]][0]\n",
    "                    query = \"Match(m:`Author`) where m.pk={0} Return m\".format(author_pk)\n",
    "                    author_node = graph.evaluate(query)\n",
    "                else:\n",
    "                    author_pk = get_random_number()\n",
    "                    author_node = Node(\"Author\", pk=author_pk, name=author[\"name\"], uri=article_[\"uri\"])\n",
    "\n",
    "                rel_article_author = Relationship(article_node, \"writtenBy\", author_node)\n",
    "                rel_author_article = Relationship(author_node, \"hasWritten\", article_node)\n",
    "\n",
    "                graph.create(rel_article_author)\n",
    "                graph.create(rel_author_article)\n",
    "\n",
    "                authors[author[\"name\"]] = (author_pk, author_node)\n",
    "\n",
    "    print(\"[+] Done loading data to neo4j\")\n",
    "\n",
    "    entities[\"news_sources\"] = news_sources\n",
    "    entities[\"authors\"] = authors\n",
    "    entities[\"languages\"] = languages\n",
    "    entities[\"articles\"] = articles_uri\n",
    "    entities[\"people\"] = people\n",
    "    entities[\"locations\"] = locations\n",
    "    entities[\"organisations\"] = organisations\n",
    "\n",
    "    # update_entities(entities)\n",
    "\n",
    "    return entities, new_articles\n",
    "\n",
    "\n",
    "def create_graph_with_parsed_entites(articles, entities, graph, valid_entities, invalid_entities):\n",
    "    people = entities[\"people\"]\n",
    "    locations = entities[\"locations\"]\n",
    "    organisations = entities[\"organisations\"]\n",
    "    articles_uri = entities[\"articles\"]\n",
    "    news_sources = entities[\"news_sources\"]\n",
    "    languages = entities[\"languages\"]\n",
    "    authors = entities[\"authors\"]\n",
    "\n",
    "    # entity_type_map = {\"ORG\": \"organisations\", \"PERSON\": \"people\", \"GPE\": \"location\"}\n",
    "\n",
    "    \n",
    "    # documentation for entities recognized by spacy: https://spacy.io/api/annotation#named-entities\n",
    "    # PRODUCT and WORK_OF_ART see if you can add this\n",
    "    print(\"[+] Creating nodes and relationships for people, locations and organisations\")\n",
    "    print(len(articles))\n",
    "\n",
    "    for i, article_ in enumerate(articles):\n",
    "        print(i)\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(\"\\tEntering data for article:\", article_[\"uri\"], \"- title:\", article_[\"title\"])\n",
    "\n",
    "        article_pk = articles_uri[article_[\"uri\"]][0]\n",
    "\n",
    "        text_nlp = nlp(article_[\"body\"])\n",
    "\n",
    "        for ent in text_nlp.ents:\n",
    "\n",
    "            if \"%\" in ent.text:\n",
    "                continue\n",
    "\n",
    "            ent_text = ent.text.lower()\n",
    "            ent_text = ent_text.replace(\"-\", \"\").replace(\"'s\", \"\")\n",
    "\n",
    "            if not check_wiki_page(ent_text):\n",
    "                continue\n",
    "\n",
    "            if ent.label_ is \"ORG\":\n",
    "                if ent_text in organisations:\n",
    "                    entity_pk = organisations[ent_text][0]\n",
    "                    query = \"Match(m:`ORGANISATION`) where m.pk={0} Return m\".format(entity_pk)\n",
    "                    entity_node = graph.evaluate(query)\n",
    "                else:\n",
    "                    entity_pk = get_random_number()\n",
    "                    entity_node = Node(\"ORGANISATION\", pk=entity_pk, name=ent_text)\n",
    "\n",
    "                query = \"Match(m:`Article`) where m.pk={0} Return m\".format(article_pk)\n",
    "                article_node = graph.evaluate(query)\n",
    "\n",
    "                rel_article_org = Relationship(article_node, \"referencesOrg\", entity_node)\n",
    "                graph.create(rel_article_org)\n",
    "\n",
    "                organisations[ent_text] = (entity_pk, entity_node)\n",
    "\n",
    "            elif ent.label_ is \"PERSON\":\n",
    "                if ent_text in people:\n",
    "                    entity_pk = people[ent_text][0]\n",
    "                    query = \"Match(m:`Person`) where m.pk={0} Return m\".format(entity_pk)\n",
    "                    entity_node = graph.evaluate(query)\n",
    "                else:\n",
    "                    entity_pk = get_random_number()\n",
    "                    entity_node = Node(\"Person\", pk=entity_pk, name=ent_text)\n",
    "\n",
    "                query = \"Match(m:`Article`) where m.pk={0} Return m\".format(article_pk)\n",
    "                article_node = graph.evaluate(query)\n",
    "\n",
    "                rel_article_person = Relationship(article_node, \"referencesPerson\", entity_node)\n",
    "                graph.create(rel_article_person)\n",
    "\n",
    "                people[ent_text] = (entity_pk, entity_node)\n",
    "\n",
    "            elif ent.label_ is \"GPE\":\n",
    "                if ent_text in locations:\n",
    "                    entity_pk = locations[ent_text][0]\n",
    "                    query = \"Match(m:`Location`) where m.pk={0} Return m\".format(entity_pk)\n",
    "                    entity_node = graph.evaluate(query)\n",
    "                else:\n",
    "                    entity_pk = get_random_number()\n",
    "                    entity_node = Node(\"Location\", pk=entity_pk, name=ent_text)\n",
    "\n",
    "                query = \"Match(m:`Article`) where m.pk={0} Return m\".format(article_pk)\n",
    "                article_node = graph.evaluate(query)\n",
    "\n",
    "                rel_article_loc = Relationship(article_node, \"referencesLoc\", entity_node)\n",
    "                graph.create(rel_article_loc)\n",
    "\n",
    "                locations[ent_text] = (entity_pk, entity_node)\n",
    "    \n",
    "            else:\n",
    "                continue\n",
    "            # valid_entities.add((article_[\"uri\"], ent.text, ent.label_))\n",
    "\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    entities[\"people\"] = people\n",
    "    entities[\"locations\"] = locations\n",
    "    entities[\"organisations\"] = organisations\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "opened-inspector",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Getting articles for keywords: antiquities trade\n",
      "<class 'dict'>\n",
      "Articles request len 28\n",
      "[+] Getting articles for keywords: illicit antiquities\n",
      "<class 'dict'>\n",
      "Articles request len 5\n",
      "[+] Getting articles for keywords: illegal antiquities\n",
      "<class 'dict'>\n",
      "Articles request len 14\n",
      "[+] Getting articles for keywords: art market\n",
      "<class 'dict'>\n",
      "Articles request len 100\n",
      "[+] Getting articles for keywords: art auction\n",
      "<class 'dict'>\n",
      "Articles request len 79\n"
     ]
    }
   ],
   "source": [
    "curr_date = datetime.today().strftime('%d-%m-%Y')\n",
    "filename = \"Daily_Articles/\" + curr_date + \".pickle\"\n",
    "outfile = open(filename,'wb')\n",
    "stuff = get_articles_request('antiquities trade')\n",
    "more_stuff = get_articles_request('illicit antiquities')\n",
    "even_more_stuff = get_articles_request('illegal antiquities')\n",
    "still_more_stuff = get_articles_request('art market')\n",
    "encore_more_stuff = get_articles_request('art auction')\n",
    "pickle.dump(stuff,outfile)\n",
    "pickle.dump(even_more_stuff,outfile)\n",
    "pickle.dump(even_more_stuff,outfile)\n",
    "pickle.dump(still_more_stuff,outfile)\n",
    "pickle.dump(encore_more_stuff,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sustained-ordinary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFound articles at: Daily_Articles/20-01-2021.pickle\n",
      "[+] Creating nodes and relationships for languages, articles, authors and content-sources\n",
      "[+] Done loading data to neo4j\n",
      "0\n",
      "[+] Creating nodes and relationships for people, locations and organisations\n",
      "0\n",
      "[+] Updating entities\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    graph = Graph(password=\"cows\")\n",
    "    valid_entities, invalid_entities = dict(), dict()\n",
    "    #clear_graph(graph)\n",
    "    articles = get_articles()\n",
    "    entities = get_all_entites()\n",
    "    time.sleep(2)\n",
    "    entities, new_articles = create_graph(articles, entities, graph)\n",
    "    print(len(new_articles))\n",
    "    entities = create_graph_with_parsed_entites(new_articles, entities, graph, valid_entities, invalid_entities)\n",
    "    update_entities(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-jonathan",
   "metadata": {},
   "source": [
    "Goddamn, it works! go over to the neo4j browser and run ```match (n) return n```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-lincoln",
   "metadata": {},
   "source": [
    "Now, I wonder if I can get the other thing (https://colab.research.google.com/drive/1-coQe5FXN30BAXTSE6Rt3fKRAgdEuoUl?usp=sharing) to process the actual text? this seems to only parse the existing metadata. Note that there are sentiment scores, a bunch of different metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
