{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "micro-happening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.12\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regular-trailer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_lg==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████▍   | 732.8 MB 1.2 MB/s eta 0:01:200  |▎                               | 7.6 MB 1.1 MB/s eta 0:12:15     |▋                               | 15.0 MB 696 kB/s eta 0:19:26     |▋                               | 16.0 MB 700 kB/s eta 0:19:19     |▊                               | 17.9 MB 643 kB/s eta 0:20:58     |▊                               | 19.0 MB 796 kB/s eta 0:16:55     |▊                               | 19.2 MB 796 kB/s eta 0:16:55     |█                               | 24.4 MB 1.3 MB/s eta 0:10:04     |█▏                              | 30.4 MB 343 kB/s eta 0:38:37     |█▎                              | 32.5 MB 545 kB/s eta 0:24:16     |█▍                              | 35.1 MB 247 kB/s eta 0:53:22     |█▍                              | 35.6 MB 503 kB/s eta 0:26:13     |█▌                              | 39.5 MB 499 kB/s eta 0:26:17     |█▋                              | 40.5 MB 322 kB/s eta 0:40:36     |█▉                              | 46.7 MB 401 kB/s eta 0:32:23     |█▉                              | 48.7 MB 110 kB/s eta 1:57:34     |██                              | 52.5 MB 358 kB/s eta 0:36:03     |██                              | 54.3 MB 334 kB/s eta 0:38:28     |██▏                             | 55.3 MB 243 kB/s eta 0:52:51     |██▎                             | 59.2 MB 159 kB/s eta 1:20:05     |██▍                             | 60.3 MB 104 kB/s eta 2:02:09     |██▌                             | 64.4 MB 28 kB/s eta 7:18:17     |██▉                             | 72.8 MB 1.3 MB/s eta 0:09:50     |██▉                             | 73.5 MB 695 kB/s eta 0:18:04     |██▉                             | 73.6 MB 695 kB/s eta 0:18:04     |██▉                             | 74.3 MB 1.6 MB/s eta 0:07:53     |███                             | 79.2 MB 447 kB/s eta 0:27:51     |███▏                            | 81.8 MB 769 kB/s eta 0:16:08     |███▎                            | 83.4 MB 456 kB/s eta 0:27:10     |███▎                            | 83.4 MB 456 kB/s eta 0:27:10     |███▎                            | 83.7 MB 910 kB/s eta 0:13:37     |███▎                            | 85.6 MB 702 kB/s eta 0:17:35     |███▎                            | 86.1 MB 702 kB/s eta 0:17:35     |███▉                            | 99.5 MB 608 kB/s eta 0:19:57     |████                            | 101.3 MB 755 kB/s eta 0:16:01     |████                            | 106.0 MB 1.6 MB/s eta 0:07:22     |████▊                           | 122.6 MB 801 kB/s eta 0:14:39     |████▉                           | 124.3 MB 581 kB/s eta 0:20:08     |█████▏                          | 132.8 MB 775 kB/s eta 0:14:56     |█████▏                          | 134.1 MB 775 kB/s eta 0:14:54     |█████▍                          | 140.0 MB 714 kB/s eta 0:16:02     |█████▌                          | 143.1 MB 1.5 MB/s eta 0:07:51     |█████▊                          | 147.1 MB 1.7 MB/s eta 0:06:52     |█████▉                          | 150.4 MB 77 kB/s eta 2:24:49     |██████                          | 154.5 MB 930 kB/s eta 0:12:03     |██████▎                         | 162.7 MB 486 kB/s eta 0:22:46     |██████▍                         | 165.9 MB 436 kB/s eta 0:25:14     |██████▋                         | 169.9 MB 684 kB/s eta 0:16:00     |███████▏                        | 183.9 MB 875 kB/s eta 0:12:15     |███████▍                        | 191.1 MB 999 kB/s eta 0:10:37     |███████▋                        | 196.3 MB 478 kB/s eta 0:22:00     |████████                        | 206.3 MB 728 kB/s eta 0:14:12     |████████                        | 208.7 MB 1.1 MB/s eta 0:09:27     |████████                        | 209.3 MB 625 kB/s eta 0:16:28     |█████████▍                      | 243.6 MB 1.0 MB/s eta 0:09:34     |██████████                      | 260.8 MB 412 kB/s eta 0:22:54     |██████████▏                     | 262.0 MB 869 kB/s eta 0:10:50     |██████████▎                     | 266.0 MB 613 kB/s eta 0:15:15     |██████████▍                     | 268.5 MB 823 kB/s eta 0:11:19     |██████████▌                     | 271.2 MB 437 kB/s eta 0:21:12     |███████████▍                    | 293.7 MB 673 kB/s eta 0:13:13     |███████████▍                    | 294.2 MB 761 kB/s eta 0:11:40     |███████████▍                    | 295.5 MB 575 kB/s eta 0:15:25     |███████████▊                    | 302.2 MB 1.0 MB/s eta 0:08:37     |████████████                    | 309.4 MB 859 kB/s eta 0:10:03     |████████████▍                   | 319.0 MB 1.3 MB/s eta 0:06:36     |████████████▍                   | 319.4 MB 762 kB/s eta 0:11:06     |████████████▌                   | 322.1 MB 547 kB/s eta 0:15:23     |████████████▉                   | 332.1 MB 345 kB/s eta 0:23:53     |████████████▉                   | 332.6 MB 465 kB/s eta 0:17:42     |█████████████                   | 333.3 MB 1.0 MB/s eta 0:08:13     |█████████████                   | 335.2 MB 1.6 MB/s eta 0:05:06     |█████████████                   | 337.8 MB 942 kB/s eta 0:08:39     |██████████████▎                 | 368.1 MB 842 kB/s eta 0:09:05     |██████████████▋                 | 376.6 MB 637 kB/s eta 0:11:47     |███████████████▎                | 394.1 MB 409 kB/s eta 0:17:37     |███████████████▌                | 400.5 MB 354 kB/s eta 0:20:05     |███████████████▊                | 406.5 MB 1.0 MB/s eta 0:06:59     |████████████████                | 412.7 MB 358 kB/s eta 0:19:15     |████████████████                | 415.8 MB 451 kB/s eta 0:15:11     |████████████████▎               | 421.1 MB 570 kB/s eta 0:11:52     |████████████████▌               | 427.1 MB 1.6 MB/s eta 0:04:10     |█████████████████               | 437.4 MB 772 kB/s eta 0:08:24     |█████████████████               | 437.7 MB 772 kB/s eta 0:08:24     |█████████████████               | 440.8 MB 896 kB/s eta 0:07:11     |█████████████████▌              | 452.2 MB 1.6 MB/s eta 0:03:51     |███████████████████             | 488.1 MB 916 kB/s eta 0:06:10     |███████████████████▌            | 504.4 MB 1.2 MB/s eta 0:04:20     |████████████████████            | 517.7 MB 1.3 MB/s eta 0:03:56     |████████████████████▏           | 521.7 MB 1.0 MB/s eta 0:04:59     |████████████████████▌           | 530.9 MB 1.1 MB/s eta 0:04:19     |████████████████████▋           | 533.5 MB 146 kB/s eta 0:33:19     |████████████████████▊           | 536.7 MB 557 kB/s eta 0:08:41     |█████████████████████           | 539.9 MB 401 kB/s eta 0:11:56     |██████████████████████▉         | 590.3 MB 548 kB/s eta 0:07:12     |██████████████████████▉         | 591.3 MB 1.4 MB/s eta 0:02:50     |███████████████████████         | 592.5 MB 889 kB/s eta 0:04:24��███████▋        | 609.7 MB 697 kB/s eta 0:05:12��████████        | 618.3 MB 1.1 MB/s eta 0:03:13��████████        | 622.8 MB 1.4 MB/s eta 0:02:29     |████████████████████████▍       | 631.2 MB 429 kB/s eta 0:07:37     |████████████████████████▌       | 632.6 MB 994 kB/s eta 0:03:16     |████████████████████████▋       | 636.4 MB 497 kB/s eta 0:06:23     |█████████████████████████       | 647.5 MB 410 kB/s eta 0:07:18     |█████████████████████████▌      | 660.3 MB 503 kB/s eta 0:05:32     |█████████████████████████▊      | 663.8 MB 123 kB/s eta 0:22:01     |█████████████████████████▉      | 667.2 MB 356 kB/s eta 0:07:28     |█████████████████████████▉      | 667.3 MB 356 kB/s eta 0:07:28     |█████████████████████████▉      | 667.9 MB 802 kB/s eta 0:03:19     |██████████████████████████      | 674.7 MB 1.2 MB/s eta 0:02:03     |██████████████████████████▍     | 681.7 MB 1.1 MB/s eta 0:02:15     |██████████████████████████▉     | 692.6 MB 559 kB/s eta 0:04:01 |███████████████████████████     | 696.7 MB 721 kB/s eta 0:03:01     |███████████████████████████     | 698.8 MB 1.5 MB/s eta 0:01:25     |███████████████████████████▍    | 708.4 MB 543 kB/s eta 0:03:39     |███████████████████████████▌    | 710.7 MB 330 kB/s eta 0:05:52     |████████████████████████████    | 723.7 MB 1.5 MB/s eta 0:01:10     |████████████████████████████▍   | 732.8 MB 1.2 MB/s eta 0:01:20\r",
      "\u001b[K     |████████████████████████████▍   | 732.8 MB 1.2 MB/s eta 0:01:20\r",
      "\u001b[K     |████████████████████████████▍   | 732.8 MB 1.2 MB/s eta 0:01:20\r",
      "\u001b[K     |████████████████████████████▍   | 732.8 MB 1.2 MB/s eta 0:01:20\r",
      "\u001b[K     |████████████████████████████▍   | 732.8 MB 1.2 MB/s eta 0:01:20\r",
      "\u001b[K     |████████████████████████████▍   | 732.9 MB 1.2 MB/s eta 0:01:20\r",
      "\u001b[K     |████████████████████████████▍   | 732.9 MB 1.2 MB/s eta 0:01:20\r",
      "\u001b[K     |████████████████████████████▍   | 732.9 MB 1.2 MB/s eta 0:01:20\r",
      "\u001b[K     |████████████████████████████▍   | 732.9 MB 1.2 MB/s eta 0:01:20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 826.9 MB 637 kB/s eta 0:00:01     |████████████████████████████▊   | 741.8 MB 142 kB/s eta 0:09:57     |█████████████████████████████   | 751.4 MB 821 kB/s eta 0:01:32     |█████████████████████████████▏  | 752.6 MB 266 kB/s eta 0:04:39     |█████████████████████████████▍  | 758.9 MB 1.5 MB/s eta 0:00:45     |█████████████████████████████▍  | 759.2 MB 1.5 MB/s eta 0:00:45     |█████████████████████████████▍  | 760.8 MB 1.1 MB/s eta 0:00:59     |██████████████████████████████▏ | 778.2 MB 704 kB/s eta 0:01:10     |██████████████████████████████▏ | 778.7 MB 768 kB/s eta 0:01:03     |███████████████████████████████▏| 804.1 MB 111 kB/s eta 0:03:26     |███████████████████████████████▋| 817.2 MB 717 kB/s eta 0:00:14     |███████████████████████████████▉| 822.4 MB 1.0 MB/s eta 0:00:05\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-poison",
   "metadata": {},
   "source": [
    "need to install py2neo, requests, spacy, en_core_web_sm. Then also get an eventregistry.org api key\n",
    "\n",
    "Remove key before git commit. Put key back before running.\n",
    "\n",
    "install pandas for dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-capitol",
   "metadata": {},
   "source": [
    "Following along with this: https://medium.com/analytics-vidhya/build-a-knowledge-graph-using-neo4j-eb7490799f4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "early-worst",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/opt/anaconda3/envs/yates/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-2.3.1/vocab/lexemes.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-346fed1dd50c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0marticle_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/yates/lib/python3.6/site-packages/en_core_web_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/yates/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, **overrides)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/yates/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/yates/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude, disable)\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/yates/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/yates/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mdeserializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0mdeserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meta.json\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrsly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0mdeserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_fix_pretrained_vectors_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0mdeserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokenizer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/yates/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         return io.open(str(self), mode, buffering, encoding, errors, newline,\n\u001b[0;32m-> 1183\u001b[0;31m                        opener=self._opener)\n\u001b[0m\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/yates/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o666\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/yates/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(pathobj, *args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/opt/anaconda3/envs/yates/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-2.3.1/vocab/lexemes.bin'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "from py2neo import database, Graph, Node, Relationship\n",
    "import re\n",
    "import en_core_web_sm\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "article_uri = dict()\n",
    "URL = \"http://eventregistry.org/api/v1/article/getArticles\"\n",
    "KEYWORDS = [[\"antiquities trade\", \"illicit antiquities\", \"illegal antiquties\", \"art trade\", \"art market\"]]\n",
    "\n",
    "\n",
    "def clear_graph(graph):\n",
    "    print(\"[+] Clearing previous graph\")\n",
    "    # graph.cypher.execute(\"MATCH (A) -[R] -> () DELETE A, R\")\n",
    "    # graph.cypher.execute(\"MATCH (A) DELETE A\")\n",
    "    graph.delete_all()\n",
    "    file_name = \"Entities/entities.json\"\n",
    "    if os.path.isfile(file_name):\n",
    "        os.remove(file_name)\n",
    "\n",
    "\n",
    "def get_all_entites():\n",
    "    entities = dict()\n",
    "\n",
    "    entities[\"news_sources\"] = dict()\n",
    "    entities[\"authors\"] = dict()\n",
    "    entities[\"languages\"] = dict()\n",
    "    entities[\"organisations\"] = dict()\n",
    "    entities[\"locations\"] = dict()\n",
    "    entities[\"people\"] = dict()\n",
    "    entities[\"articles\"] = dict()\n",
    "\n",
    "    file_name = \"Entities/entities.json\"\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name) as f:\n",
    "            entities = json.load(f)\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def get_articles_request(keywords, num_articles=100):\n",
    "    print(\"[+] Getting articles for keywords:\", keywords)\n",
    "    articles = []\n",
    "\n",
    "    \n",
    "    # for keywords in KEYWORDS:\n",
    "    payload = {\"action\": \"getArticles\", \"keyword\": keywords, \"articlesPage\": 1, \"articlesSortBy\": \"date\",\n",
    "               \"articlesSortByAsc\": False, \"articlesArticleBodyLen\": -1, \"resultType\": \"articles\",\n",
    "               \"dataType\": [\"news\", \"pr\"], \"apiKey\": \"9ab7140b-1113-4329-a560-97dfb9970cde\",\n",
    "               \"forceMaxDataTimeWindow\": 31,\n",
    "               \"articlesCount\": num_articles}\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", URL, headers=headers, data=json.dumps(payload))\n",
    "    try:\n",
    "        articles = json.loads(response.text.encode('utf8'))\n",
    "    except:\n",
    "        print(\"Exception occured for keyword:\", keywords)\n",
    "        return list()\n",
    "    # print(articles.keys())\n",
    "    print(type(articles))\n",
    "    print(\"Articles request len\", len(articles[\"articles\"][\"results\"]))\n",
    "    return articles[\"articles\"][\"results\"]\n",
    "\n",
    "\n",
    "def get_articles(num_articles=100):\n",
    "    curr_date = datetime.today().strftime('%d-%m-%Y')\n",
    "    #curr_date = \"29 -05-2020\"\n",
    "    file_name = \"Daily_Articles/\" + curr_date + \".pickle\"\n",
    "\n",
    "    if not os.path.isfile(file_name):\n",
    "        print(\"\\tCould not find articles at: {file_name}, requesting\".format(file_name=file_name))\n",
    "        with open(file_name, \"wb\") as handle:\n",
    "            articles = list()\n",
    "            for keywords in KEYWORDS[1]:\n",
    "                articles += get_articles_request(keywords, num_articles)\n",
    "            articles_dict = dict()\n",
    "            articles_dict[\"articles\"] = {\"results\":articles}\n",
    "            pickle.dump(articles_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            return articles_dict #i changed this b/c of twitter guy\n",
    "    else:\n",
    "        print(\"\\tFound articles at: {file_name}\".format(file_name=file_name))\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "\n",
    "\n",
    "def update_entities(entities):\n",
    "    print(\"[+] Updating entities\")\n",
    "\n",
    "    # with open('Entities/entities.pkl', 'wb') as output:\n",
    "    #     pickle.dump(entities, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    file_name = \"Entities/entities.json\"\n",
    "    with open(file_name, 'w') as fp:\n",
    "        json.dump(entities, fp)\n",
    "\n",
    "\n",
    "def get_random_number():\n",
    "    return random.randint(100000000, 1000000000)\n",
    "\n",
    "\n",
    "def check_wiki_page(query):\n",
    "    try:\n",
    "        result = requests.get('https://en.wikipedia.org/wiki/{0}'.format(query), verify=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if result.status_code == 200:  # the article exists\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def create_graph(articles, entities, graph):\n",
    "    count_ = 0\n",
    "    articles_uri = entities[\"articles\"]\n",
    "    news_sources = entities[\"news_sources\"]\n",
    "    languages = entities[\"languages\"]\n",
    "    authors = entities[\"authors\"]\n",
    "    people = entities[\"people\"]\n",
    "    locations = entities[\"locations\"]\n",
    "    organisations = entities[\"organisations\"]\n",
    "\n",
    "    new_articles = []\n",
    "\n",
    "    print(\"[+] Creating nodes and relationships for languages, articles, authors and content-sources\")\n",
    "\n",
    "    for article_ in articles:#[\"articles\"][\"results\"]:\n",
    "        count_ += 1\n",
    "        if article_[\"uri\"] not in articles_uri:\n",
    "            new_articles.append(article_)\n",
    "            print(\"\\tEntering data for article:\", article_[\"uri\"], \"- title:\", article_[\"title\"])\n",
    "\n",
    "            # Creating Nodes\n",
    "            article_pk = get_random_number()\n",
    "            article_node = Node(\"Article\", pk=article_pk, name=article_['title'], sentiment=article_[\"sentiment\"],\n",
    "                                text=article_[\"body\"],\n",
    "                                dtype=article_[\"dataType\"], published_on=article_[\"dateTimePub\"], uri=article_[\"uri\"],\n",
    "                                url=article_[\"url\"])\n",
    "\n",
    "            articles_uri[article_[\"uri\"]] = (article_pk, article_node)\n",
    "\n",
    "            # Entering article\n",
    "            if article_[\"source\"][\"title\"] in news_sources:\n",
    "                source_node_pk = news_sources[article_[\"source\"][\"title\"]][0]\n",
    "                query = \"Match(m:`Content-Source`) where m.pk={0} Return m\".format(source_node_pk)\n",
    "                source_node = graph.evaluate(query)\n",
    "            else:\n",
    "                source_node_pk = get_random_number()\n",
    "                source_node = Node(\"Content-Source\", pk=source_node_pk, name=article_[\"source\"][\"title\"],\n",
    "                                   uri=article_[\"source\"][\"uri\"])\n",
    "\n",
    "            rel_article_source = Relationship(article_node, \"publishedBy\", source_node)\n",
    "            graph.create(rel_article_source)\n",
    "            news_sources[article_[\"source\"][\"title\"]] = (source_node_pk, source_node)\n",
    "\n",
    "            #Entering language\n",
    "            if article_[\"lang\"] in languages:\n",
    "                lang_pk = languages[article_[\"lang\"]][0]\n",
    "                query = \"Match(m:`Language`) where m.pk={0} Return m\".format(lang_pk)\n",
    "                lang_node = graph.evaluate(query)\n",
    "            else:\n",
    "                lang_pk = get_random_number()\n",
    "                lang_node = Node(\"Language\", pk=lang_pk, name=article_[\"lang\"])\n",
    "            \n",
    "            rel_article_lang = Relationship(article_node, \"language\", lang_node)\n",
    "            graph.create(rel_article_lang)\n",
    "            languages[article_[\"lang\"]] = (lang_pk, lang_node)\n",
    "\n",
    "            # Entering authors\n",
    "            for author in article_[\"authors\"]:\n",
    "                if author[\"name\"] in authors:\n",
    "                    author_pk = authors[author[\"name\"]][0]\n",
    "                    query = \"Match(m:`Author`) where m.pk={0} Return m\".format(author_pk)\n",
    "                    author_node = graph.evaluate(query)\n",
    "                else:\n",
    "                    author_pk = get_random_number()\n",
    "                    author_node = Node(\"Author\", pk=author_pk, name=author[\"name\"], uri=article_[\"uri\"])\n",
    "\n",
    "                rel_article_author = Relationship(article_node, \"writtenBy\", author_node)\n",
    "                rel_author_article = Relationship(author_node, \"hasWritten\", article_node)\n",
    "\n",
    "                graph.create(rel_article_author)\n",
    "                graph.create(rel_author_article)\n",
    "\n",
    "                authors[author[\"name\"]] = (author_pk, author_node)\n",
    "\n",
    "    print(\"[+] Done loading data to neo4j\")\n",
    "\n",
    "    entities[\"news_sources\"] = news_sources\n",
    "    entities[\"authors\"] = authors\n",
    "    entities[\"languages\"] = languages\n",
    "    entities[\"articles\"] = articles_uri\n",
    "    entities[\"people\"] = people\n",
    "    entities[\"locations\"] = locations\n",
    "    entities[\"organisations\"] = organisations\n",
    "\n",
    "    # update_entities(entities)\n",
    "\n",
    "    return entities, new_articles\n",
    "\n",
    "\n",
    "def create_graph_with_parsed_entites(articles, entities, graph, valid_entities, invalid_entities):\n",
    "    people = entities[\"people\"]\n",
    "    locations = entities[\"locations\"]\n",
    "    organisations = entities[\"organisations\"]\n",
    "    articles_uri = entities[\"articles\"]\n",
    "    news_sources = entities[\"news_sources\"]\n",
    "    languages = entities[\"languages\"]\n",
    "    authors = entities[\"authors\"]\n",
    "\n",
    "    # entity_type_map = {\"ORG\": \"organisations\", \"PERSON\": \"people\", \"GPE\": \"location\"}\n",
    "\n",
    "    \n",
    "    # documentation for entities recognized by spacy: https://spacy.io/api/annotation#named-entities\n",
    "    # PRODUCT and WORK_OF_ART see if you can add this\n",
    "    print(\"[+] Creating nodes and relationships for people, locations and organisations\")\n",
    "    print(len(articles))\n",
    "\n",
    "    for i, article_ in enumerate(articles):\n",
    "        print(i)\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(\"\\tEntering data for article:\", article_[\"uri\"], \"- title:\", article_[\"title\"])\n",
    "\n",
    "        article_pk = articles_uri[article_[\"uri\"]][0]\n",
    "\n",
    "        text_nlp = nlp(article_[\"body\"])\n",
    "\n",
    "        for ent in text_nlp.ents:\n",
    "\n",
    "            if \"%\" in ent.text:\n",
    "                continue\n",
    "\n",
    "            ent_text = ent.text.lower()\n",
    "            ent_text = ent_text.replace(\"-\", \"\").replace(\"'s\", \"\")\n",
    "\n",
    "            if not check_wiki_page(ent_text):\n",
    "                continue\n",
    "\n",
    "            if ent.label_ is \"ORG\":\n",
    "                if ent_text in organisations:\n",
    "                    entity_pk = organisations[ent_text][0]\n",
    "                    query = \"Match(m:`ORGANISATION`) where m.pk={0} Return m\".format(entity_pk)\n",
    "                    entity_node = graph.evaluate(query)\n",
    "                else:\n",
    "                    entity_pk = get_random_number()\n",
    "                    entity_node = Node(\"ORGANISATION\", pk=entity_pk, name=ent_text)\n",
    "\n",
    "                query = \"Match(m:`Article`) where m.pk={0} Return m\".format(article_pk)\n",
    "                article_node = graph.evaluate(query)\n",
    "\n",
    "                rel_article_org = Relationship(article_node, \"referencesOrg\", entity_node)\n",
    "                graph.create(rel_article_org)\n",
    "\n",
    "                organisations[ent_text] = (entity_pk, entity_node)\n",
    "\n",
    "            elif ent.label_ is \"PERSON\":\n",
    "                if ent_text in people:\n",
    "                    entity_pk = people[ent_text][0]\n",
    "                    query = \"Match(m:`Person`) where m.pk={0} Return m\".format(entity_pk)\n",
    "                    entity_node = graph.evaluate(query)\n",
    "                else:\n",
    "                    entity_pk = get_random_number()\n",
    "                    entity_node = Node(\"Person\", pk=entity_pk, name=ent_text)\n",
    "\n",
    "                query = \"Match(m:`Article`) where m.pk={0} Return m\".format(article_pk)\n",
    "                article_node = graph.evaluate(query)\n",
    "\n",
    "                rel_article_person = Relationship(article_node, \"referencesPerson\", entity_node)\n",
    "                graph.create(rel_article_person)\n",
    "\n",
    "                people[ent_text] = (entity_pk, entity_node)\n",
    "\n",
    "            elif ent.label_ is \"GPE\":\n",
    "                if ent_text in locations:\n",
    "                    entity_pk = locations[ent_text][0]\n",
    "                    query = \"Match(m:`Location`) where m.pk={0} Return m\".format(entity_pk)\n",
    "                    entity_node = graph.evaluate(query)\n",
    "                else:\n",
    "                    entity_pk = get_random_number()\n",
    "                    entity_node = Node(\"Location\", pk=entity_pk, name=ent_text)\n",
    "\n",
    "                query = \"Match(m:`Article`) where m.pk={0} Return m\".format(article_pk)\n",
    "                article_node = graph.evaluate(query)\n",
    "\n",
    "                rel_article_loc = Relationship(article_node, \"referencesLoc\", entity_node)\n",
    "                graph.create(rel_article_loc)\n",
    "\n",
    "                locations[ent_text] = (entity_pk, entity_node)\n",
    "    \n",
    "            else:\n",
    "                continue\n",
    "            # valid_entities.add((article_[\"uri\"], ent.text, ent.label_))\n",
    "\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    entities[\"people\"] = people\n",
    "    entities[\"locations\"] = locations\n",
    "    entities[\"organisations\"] = organisations\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "functioning-fundamental",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_articles_request' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8676bd3a0c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Daily_Articles/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurr_date\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_articles_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'antiquities trade'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmore_stuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_articles_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'illicit antiquities'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0meven_more_stuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_articles_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'illegal antiquities'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_articles_request' is not defined"
     ]
    }
   ],
   "source": [
    "curr_date = datetime.today().strftime('%d-%m-%Y')\n",
    "filename = \"Daily_Articles/\" + curr_date + \".pickle\"\n",
    "outfile = open(filename,'wb')\n",
    "stuff = get_articles_request('antiquities trade')\n",
    "more_stuff = get_articles_request('illicit antiquities')\n",
    "even_more_stuff = get_articles_request('illegal antiquities')\n",
    "still_more_stuff = get_articles_request('art market')\n",
    "encore_more_stuff = get_articles_request('art auction')\n",
    "sothebys_more_stuff = get_articles_request('sothebys')\n",
    "christies_more_stuff = get_articles_request('christies')\n",
    "pickle.dump(stuff,outfile)\n",
    "pickle.dump(even_more_stuff,outfile)\n",
    "pickle.dump(even_more_stuff,outfile)\n",
    "pickle.dump(still_more_stuff,outfile)\n",
    "pickle.dump(encore_more_stuff,outfile)\n",
    "pickle.dump(sothebys_more_stuff,outfile)\n",
    "pickle.dump(christies_more_stuff,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "parental-luxury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFound articles at: Daily_Articles/10-02-2021.pickle\n",
      "[+] Creating nodes and relationships for languages, articles, authors and content-sources\n",
      "\tEntering data for article: 6425348305 - title: Forget bandages: This first-ever Egyptian mud-wrapped mummy dates\n",
      "\tEntering data for article: 6425232200 - title: Khaled al-Asaad: Possible remains of Syrian archaeologist found at Palmyra * The Syrian Observatory For Human Rights\n",
      "\tEntering data for article: 6424852820 - title: Khaled al-Asaad: Possible remains of Syrian archaeologist found at Palmyra\n",
      "\tEntering data for article: 6421553646 - title: Museum of the Bible Returns Looted Artifacts to Egypt and Iraq\n",
      "\tEntering data for article: 6420093308 - title: Who Changed The Bodies and What is The Real Name of The Egyptian Mummy? | Al Bawaba\n",
      "\tEntering data for article: 6419464515 - title: Rare Ancient Egyptian 'mud mummy' may have been in wrong coffin all this time\n",
      "\tEntering data for article: 6419453281 - title: Rare Ancient Egyptian 'mud mummy' may have been in wrong coffin all this time\n",
      "\tEntering data for article: 6419413656 - title: Rare Ancient Egyptian 'mud mummy' may have been in wrong coffin all this time\n",
      "\tEntering data for article: 6419282994 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "\tEntering data for article: 6418687266 - title: Ancient Mummy Found Entombed in Strange Cocoon Never Seen by Archeologists\n",
      "\tEntering data for article: 6418620734 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "\tEntering data for article: 6418580573 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "\tEntering data for article: 6418569945 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "\tEntering data for article: 6418564556 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "\tEntering data for article: 6418552837 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "\tEntering data for article: 6418537016 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "\tEntering data for article: 6418534031 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "\tEntering data for article: 6418472639 - title: In a case of potential mistaken mummy identity, scientists uncover clues | NewsChannel 3-12\n",
      "\tEntering data for article: 6418455377 - title: Egyptian mummy 'doesn't match the name on 3,000-year-old coffin'\n",
      "\tEntering data for article: 6418440044 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "\tEntering data for article: 6418422191 - title: In a case of potential mistaken mummy identity, scientists uncover clues - KION546\n",
      "\tEntering data for article: 6418412534 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "\tEntering data for article: 6418396407 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "\tEntering data for article: 6418364051 - title: Never-before-seen 'mud mummy' from Egypt discovered in wrong coffin\n",
      "\tEntering data for article: 6409927510 - title: A momentous victory for global anti-corruption efforts\n",
      "\tEntering data for article: 6400732039 - title: Defense Act Extends Anti-Money-Laundering Requirements to Antiquities Market \n",
      "[+] Done loading data to neo4j\n",
      "26\n",
      "[+] Creating nodes and relationships for people, locations and organisations\n",
      "26\n",
      "0\n",
      "\tEntering data for article: 6425348305 - title: Forget bandages: This first-ever Egyptian mud-wrapped mummy dates\n",
      "--- 18.152644872665405 seconds ---\n",
      "1\n",
      "\tEntering data for article: 6425232200 - title: Khaled al-Asaad: Possible remains of Syrian archaeologist found at Palmyra * The Syrian Observatory For Human Rights\n",
      "--- 18.547553777694702 seconds ---\n",
      "2\n",
      "\tEntering data for article: 6424852820 - title: Khaled al-Asaad: Possible remains of Syrian archaeologist found at Palmyra\n",
      "--- 11.778843879699707 seconds ---\n",
      "3\n",
      "\tEntering data for article: 6421553646 - title: Museum of the Bible Returns Looted Artifacts to Egypt and Iraq\n",
      "--- 19.9505558013916 seconds ---\n",
      "4\n",
      "\tEntering data for article: 6420093308 - title: Who Changed The Bodies and What is The Real Name of The Egyptian Mummy? | Al Bawaba\n",
      "--- 18.57215690612793 seconds ---\n",
      "5\n",
      "\tEntering data for article: 6419464515 - title: Rare Ancient Egyptian 'mud mummy' may have been in wrong coffin all this time\n",
      "--- 10.20179796218872 seconds ---\n",
      "6\n",
      "\tEntering data for article: 6419453281 - title: Rare Ancient Egyptian 'mud mummy' may have been in wrong coffin all this time\n",
      "--- 8.493702173233032 seconds ---\n",
      "7\n",
      "\tEntering data for article: 6419413656 - title: Rare Ancient Egyptian 'mud mummy' may have been in wrong coffin all this time\n",
      "--- 8.436983108520508 seconds ---\n",
      "8\n",
      "\tEntering data for article: 6419282994 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "--- 19.64494276046753 seconds ---\n",
      "9\n",
      "\tEntering data for article: 6418687266 - title: Ancient Mummy Found Entombed in Strange Cocoon Never Seen by Archeologists\n",
      "--- 18.383455991744995 seconds ---\n",
      "10\n",
      "\tEntering data for article: 6418620734 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "--- 11.721930980682373 seconds ---\n",
      "11\n",
      "\tEntering data for article: 6418580573 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "--- 12.284746885299683 seconds ---\n",
      "12\n",
      "\tEntering data for article: 6418569945 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "--- 12.11457896232605 seconds ---\n",
      "13\n",
      "\tEntering data for article: 6418564556 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "--- 11.67150092124939 seconds ---\n",
      "14\n",
      "\tEntering data for article: 6418552837 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "--- 12.2410569190979 seconds ---\n",
      "15\n",
      "\tEntering data for article: 6418537016 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "--- 14.361331939697266 seconds ---\n",
      "16\n",
      "\tEntering data for article: 6418534031 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "--- 11.779216289520264 seconds ---\n",
      "17\n",
      "\tEntering data for article: 6418472639 - title: In a case of potential mistaken mummy identity, scientists uncover clues | NewsChannel 3-12\n",
      "--- 11.852247953414917 seconds ---\n",
      "18\n",
      "\tEntering data for article: 6418455377 - title: Egyptian mummy 'doesn't match the name on 3,000-year-old coffin'\n",
      "--- 18.76803207397461 seconds ---\n",
      "19\n",
      "\tEntering data for article: 6418440044 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "--- 13.935745000839233 seconds ---\n",
      "20\n",
      "\tEntering data for article: 6418422191 - title: In a case of potential mistaken mummy identity, scientists uncover clues - KION546\n",
      "--- 12.272605180740356 seconds ---\n",
      "21\n",
      "\tEntering data for article: 6418412534 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "--- 13.296074151992798 seconds ---\n",
      "22\n",
      "\tEntering data for article: 6418396407 - title: In a case of potential mistaken mummy identity, scientists uncover clues\n",
      "--- 12.110514163970947 seconds ---\n",
      "23\n",
      "\tEntering data for article: 6418364051 - title: Never-before-seen 'mud mummy' from Egypt discovered in wrong coffin\n",
      "--- 15.193455934524536 seconds ---\n",
      "24\n",
      "\tEntering data for article: 6409927510 - title: A momentous victory for global anti-corruption efforts\n",
      "--- 34.146024227142334 seconds ---\n",
      "25\n",
      "\tEntering data for article: 6400732039 - title: Defense Act Extends Anti-Money-Laundering Requirements to Antiquities Market \n",
      "--- 12.492285966873169 seconds ---\n",
      "[+] Updating entities\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    graph = Graph(password=\"cows\")\n",
    "    valid_entities, invalid_entities = dict(), dict()\n",
    "    #clear_graph(graph)\n",
    "    articles = get_articles()\n",
    "    entities = get_all_entites()\n",
    "    time.sleep(2)\n",
    "    entities, new_articles = create_graph(articles, entities, graph)\n",
    "    print(len(new_articles))\n",
    "    entities = create_graph_with_parsed_entites(new_articles, entities, graph, valid_entities, invalid_entities)\n",
    "    update_entities(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-wound",
   "metadata": {},
   "source": [
    "Goddamn, it works! go over to the neo4j browser and run ```match (n) return n```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-happiness",
   "metadata": {},
   "source": [
    "Now, I wonder if I can get the other thing (https://colab.research.google.com/drive/1-coQe5FXN30BAXTSE6Rt3fKRAgdEuoUl?usp=sharing) to process the actual text? this seems to only parse the existing metadata. Note that there are sentiment scores, a bunch of different metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
